# knowledge-distillation-mini-lab
A concise, reproducible experiment on Knowledge Distillation for Graph Neural Networks â€” inspired by model extraction attack studies. Includes teacher-student training, hard &amp; soft distillation, and performance comparison on Colab.
